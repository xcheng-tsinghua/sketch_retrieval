"""
用于解决笔划末端不重合的问题
"""
import einops
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from einops import rearrange

# import global_defs
import sdgraph.utils as eu
from sdgraph.attn_3dgcn import AttnGCN3D


class SinusoidalPosEmb(nn.Module):
    """
    将时间步t转化为embedding
    """
    def __init__(self, dim, theta):  # 256， 10000
        super().__init__()
        self.dim = dim
        self.theta = theta

    def forward(self, x):
        """
        :param x: 时间步 [bs, ]
        :return:
        """
        device = x.device
        half_dim = self.dim // 2  # 128
        emb = math.log(self.theta) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
        emb = x[:, None] * emb[None, :]
        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
        return emb


class TimeEncode(nn.Module):
    """
    编码时间步
    """
    def __init__(self, channel_time):
        super().__init__()
        self.encoder = nn.Sequential(
            SinusoidalPosEmb(channel_time // 4, theta=10000),
            nn.Linear(channel_time // 4, channel_time // 2),
            eu.activate_func(),
            nn.Linear(channel_time // 2, channel_time)
        )

    def forward(self, x):
        x = self.encoder(x)
        return x


class TimeMerge(nn.Module):
    def __init__(self, dim_in, dim_out, time_emb_dim, dropout=0.0):
        super().__init__()
        self.mlp = nn.Sequential(
            eu.activate_func(),
            nn.Linear(time_emb_dim, dim_out * 2)
        )

        self.block1 = Block(dim_in, dim_out, dropout=dropout)
        self.block2 = Block(dim_out, dim_out)
        self.res_conv = nn.Conv2d(dim_in, dim_out, 1) if dim_in != dim_out else nn.Identity()

    def forward(self, x, time_emb):
        """
        将输入的 tensor 融合时间特征
        :param x: [bs, emb, n_stk] or [bs, emb, n_stk, n_stk_pnt]
        :param time_emb:
        :return: [bs, emb, n_stk] or [bs, emb, n_stk, n_stk_pnt]
        """
        is_stk_stk_pnt = False

        if len(x.size()) == 4:
            is_stk_stk_pnt = True
            bs, emb, n_stk, n_stk_pnt = x.size()
            x = x.view(bs, emb, n_stk * n_stk_pnt)

        time_emb = self.mlp(time_emb)
        time_emb = rearrange(time_emb, 'b c -> b c 1')
        scale_shift = time_emb.chunk(2, dim=1)

        h = self.block1(x, scale_shift=scale_shift)
        h = self.block2(h)

        x_embd = h + self.res_conv(x)

        if is_stk_stk_pnt:
            x_embd = x_embd.view(bs, x_embd.size(1), n_stk, n_stk_pnt)

        return x_embd


class Block(nn.Module):
    def __init__(self, dim, dim_out, dropout=0.):
        super().__init__()
        self.conv = nn.Conv1d(dim, dim_out, 1)
        self.norm = RMSNorm(dim_out)
        self.act = nn.SiLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, scale_shift=None):
        """
        :param x: [bs, channel, n_node]
        :param scale_shift: [bs, ]
        :return:
        """
        x = self.conv(x)
        x = self.norm(x)

        if scale_shift is not None:
            scale, shift = scale_shift
            x = x * (scale + 1) + shift

        x = self.dropout(self.act(x))
        return x
# -----------------------------------------------------------------------------------------


class RMSNorm(nn.Module):
    def __init__(self, dim):
        """
        :param dim: forward过程中输入x的特征长度
        """
        super().__init__()
        self.scale = dim ** 0.5
        self.g = nn.Parameter(torch.ones(1, dim, 1))

    def forward(self, x):
        return F.normalize(x, dim=1) * self.g * self.scale


class PointToSparse(nn.Module):
    """
    将 dense graph 的数据转移到 sparse graph'
    使用一维卷积及最大池化方式
    """
    def __init__(self, point_dim, sparse_out, with_time=False, time_emb_dim=0, dropout=0.0):
        super().__init__()

        self.with_time = with_time

        # 将 DGraph 的数据转移到 SGraph
        mid_dim = int((point_dim * sparse_out) ** 0.5)
        self.point_conv = nn.Sequential(
            nn.Conv1d(in_channels=point_dim, out_channels=mid_dim, kernel_size=3),
            nn.BatchNorm1d(mid_dim),
            eu.activate_func(),
            nn.Dropout1d(dropout),

            nn.Conv1d(in_channels=mid_dim, out_channels=sparse_out, kernel_size=3),
            nn.BatchNorm1d(sparse_out),
            eu.activate_func(),
            nn.Dropout1d(dropout),
        )

        if self.with_time:
            self.time_merge = TimeMerge(sparse_out, sparse_out, time_emb_dim)

    def forward(self, xy, time_emb=None):
        """
        :param xy: [bs, n_stk, n_stk_pnt, 2]
        :param time_emb: [bs, emb]
        :return: [bs, emb, n_stk]
        """
        n_stk = xy.size(1)

        # -> [bs, emb, n_pnts]
        xy = einops.rearrange(xy, 'b s sp c -> b c (s sp)')

        # 提升点数 -> [bs, emb, n_pnts + 4]
        xy = eu.sequence_extend(xy, 2)

        # 获取每个点的特征 -> [bs, emb, n_pnts]
        xy = self.point_conv(xy)

        # 截取每个笔划 -> [bs, emb, stk, stk_pnt]
        xy = einops.rearrange(xy, 'b c (s sp) -> b c s sp', s=n_stk)

        # 获取笔划特征 -> [bs, emb, n_stk]
        xy = xy.max(3)[0]

        assert self.with_time ^ (time_emb is None)
        if self.with_time:
            xy = self.time_merge(xy, time_emb)

        return xy


class PointToDense(nn.Module):
    """
    利用点坐标生成 dense graph
    使用DGCNN直接为每个点生成对应特征
    """
    def __init__(self, point_dim, emb_dim, with_time=False, time_emb_dim=0, n_near=10):
        super().__init__()
        self.encoder = AttnGCN3D(point_dim, 0, emb_dim, n_near, n_near)

        self.with_time = with_time
        if self.with_time:
            self.time_merge = TimeMerge(emb_dim, emb_dim, time_emb_dim)

    def forward(self, xy, time_emb=None):
        """
        :param xy: [bs, n_stk, n_stk_pnt, 2]
        :param time_emb: [bs, emb]
        :return: [bs, emb, n_stk, n_stk_pnt]
        """
        bs, n_stk, n_stk_pnt, channel = xy.size()

        xy = einops.rearrange(xy, 'b s sp c -> b c (s sp)')
        dense_emb = self.encoder(xy)

        assert self.with_time ^ (time_emb is None)
        if self.with_time:
            dense_emb = self.time_merge(dense_emb, time_emb)

        dense_emb = dense_emb.view(bs, dense_emb.size(1), n_stk, n_stk_pnt)
        return dense_emb


class SparseUpdate(nn.Module):
    """
    将 dense graph 的数据转移到 sparse graph
    通过卷积然后最大池化到一个特征，然后拼接
    """
    def __init__(self, sp_coor_channel, sp_in, sp_out, n_near=2):
        """

        :param sp_coor_channel: 稀疏图节点的坐标维度
        :param sp_in: 输入坐标维度
        :param sp_out: 输出坐标维度
        :param n_near:
        """
        super().__init__()
        self.encoder = AttnGCN3D(sp_coor_channel, sp_in, sp_out, n_near, n_near)

    def forward(self, sparse_coor, sparse_fea):
        """
        :param sparse_coor: [bs, emb, n_stk] 稀疏图节点坐标
        :param sparse_fea: [bs, emb, n_stk]
        :return: [bs, emb, n_stk]
        """
        sparse_fea = self.encoder(sparse_coor, sparse_fea)
        return sparse_fea


class DenseUpdate(nn.Module):
    """
    将 dense graph 的数据转移到 sparse graph
    通过卷积然后最大池化到一个特征，然后拼接
    """
    def __init__(self, dn_coor_channel, dn_in, dn_out, n_near=10, dropout=0.0):
        """

        :param dn_coor_channel: DGraph 中节点坐标
        :param dn_in:
        :param dn_out:
        :param n_near:
        :param dropout:
        """
        super().__init__()
        self.encoder = AttnGCN3D(dn_coor_channel, dn_in, dn_out, n_near, n_near)

    def forward(self, dense_coor, dense_fea):
        """
        :param dense_coor: [bs, emb, n_stk, n_stk_pnt]  密集图中点坐标
        :param dense_fea: [bs, emb, n_stk, n_stk_pnt]
        :return: [bs, emb, n_stk, n_stk_pnt]
        """

        bs, emb, n_stk, n_stk_pnt = dense_fea.size()

        dense_fea = dense_fea.view(bs, emb, n_stk * n_stk_pnt)
        dense_fea = self.encoder(dense_coor, dense_fea)
        dense_fea = dense_fea.view(bs, dense_fea.size(1), n_stk, n_stk_pnt)

        return dense_fea


class DownSample(nn.Module):
    def __init__(self, dim_in, dim_out, dropout=0.4):
        super().__init__()

        self.conv = nn.Sequential(
            nn.Conv1d(
                in_channels=dim_in,  # 输入通道数 (RGB)
                out_channels=dim_out,  # 输出通道数 (保持通道数不变)
                kernel_size=6,  # 卷积核大小：1x3，仅在宽度方向上滑动
                stride=2,  # 步幅：高度方向为1，宽度方向为2
            ),
            nn.BatchNorm1d(dim_out),
            eu.activate_func(),
            nn.Dropout1d(dropout)
        )

    def forward(self, dense_fea, dense_coor):
        """
        :param dense_fea: [bs, emb, n_stk, n_stk_pnt]
        :param dense_coor: [bs, emb, n_stk, n_stk_pnt]
        :return: [bs, emb, n_stk, n_stk_pnt // 2]
        """
        n_stk = dense_fea.size(2)

        # 扩充序列 -> [bs, emb, pnt]
        dense_fea = einops.rearrange(dense_fea, 'b c s sp -> b c (s sp)')

        # -> [bs, emb, pnt + 4]
        dense_fea = eu.sequence_extend(dense_fea, 2)

        # -> [bs, emb, pnt // 2]  卷积后长度: n = (N + 4 - 6) // 2 + 1 = N // 2 - 1 + 1 = N // 2
        dense_fea = self.conv(dense_fea)

        # -> [bs, emb, n_stk, n_stk_pnt // 2]
        dense_fea = einops.rearrange(dense_fea, 'b c (s sp) -> b c s sp', s=n_stk)
        dense_coor = dense_coor[..., ::2]
        return dense_fea, dense_coor


class UpSample(nn.Module):
    """
    对sdgraph同时进行上采样
    上采样后笔划数及笔划上的点数均变为原来的2倍
    """
    def __init__(self, dim_in, dim_out, dropout=0.4):
        super().__init__()

        self.conv = nn.Sequential(
            nn.ConvTranspose1d(
                in_channels=dim_in,  # 输入通道数
                out_channels=dim_out,  # 输出通道数
                kernel_size=4,  # 卷积核大小：1x2，仅在宽度方向扩展
                stride=2,  # 步幅：高度不变，宽度扩展为原来的 2 倍
                padding=3,  # 填充：在宽度方向保持有效中心对齐
            ),
            nn.BatchNorm1d(dim_out),
            eu.activate_func(),
            nn.Dropout1d(dropout)
        )

    def forward(self, dense_fea, dense_coor=None):
        """
        :param dense_fea: [bs, emb, n_stk, n_stk_pnt]
        :return: [bs, emb, n_stk, n_stk_pnt * 2]
        """
        n_stk = dense_fea.size(2)

        # 扩充序列 -> [bs, emb, pnt]
        dense_fea = einops.rearrange(dense_fea, 'b c s sp -> b c (s sp)')

        # -> [bs, emb, pnt + 2]
        dense_fea = eu.sequence_extend(dense_fea, 1)

        # 上采样 -> [bs, emb, pnt * 2]
        dense_fea = self.conv(dense_fea)

        # -> [bs, emb, n_stk, n_stk_pnt * 2]
        dense_fea = einops.rearrange(dense_fea, 'b c (s sp) -> b c s sp', s=n_stk)
        return dense_fea, None


class SparseToDense(nn.Module):
    """
    将sgraph转移到dgraph
    直接拼接到该笔划对应的点
    """
    def __init__(self, sparse_in, dense_in, dropout=0.0):
        super().__init__()

        self.encoder = eu.MLP(
            dimension=2,
            channels=(sparse_in + dense_in, sparse_in + dense_in),
            dropout=dropout,
            final_proc=True
        )

    def forward(self, sparse_fea, dense_fea):
        """
        :param dense_fea: [bs, emb, n_stk, n_stk_pnt]
        :param sparse_fea: [bs, emb, n_stk]
        :return: [bs, emb, n_stk, n_stk_pnt]
        """
        dense_feas_from_sparse = einops.repeat(sparse_fea, 'b c s -> b c s sp', sp=dense_fea.size(3))

        # -> [bs, emb, n_stk, n_stk_pnt]
        union_dense = torch.cat([dense_fea, dense_feas_from_sparse], dim=1)
        union_dense = self.encoder(union_dense)

        return union_dense


class DenseToSparse(nn.Module):
    """
    将 dense graph 的数据转移到 sparse graph
    通过卷积然后最大池化到一个特征，然后拼接
    """
    def __init__(self, sparse_in, dense_in, dropout=0.0):
        super().__init__()

        # 先利用时序进行更新
        self.temporal_encoding = nn.Sequential(
            nn.Conv1d(in_channels=dense_in, out_channels=dense_in, kernel_size=5),  # 输入 = 输出
            nn.BatchNorm1d(dense_in),
            eu.activate_func(),
            nn.Dropout1d(dropout),
        )

        self.encoder = eu.MLP(
            dimension=1,
            channels=(sparse_in + dense_in, sparse_in + dense_in),
            dropout=dropout,
            final_proc=True
        )

    def forward(self, sparse_fea, dense_fea):
        """
        :param dense_fea: [bs, emb, n_stk, n_stk_pnt]
        :param sparse_fea: [bs, emb, n_stk]
        :return: [bs, emb, n_stk]
        """
        n_stk = dense_fea.size(2)

        # 先进行时序编码 -> [bs, emb, pnt]
        dense_fea = einops.rearrange(dense_fea, 'b c s sp -> b c (s sp)')

        # 序列扩展 -> [bs, emb, pnt + 4]
        dense_fea = eu.sequence_extend(dense_fea, 2)

        # 时序编码 -> [bs, emb, pnt]
        dense_fea = self.temporal_encoding(dense_fea)

        # 截取笔划特征 -> [bs, emb, n_stk, n_stk_pnt]
        dense_fea = einops.rearrange(dense_fea, 'b c (s sp) -> b c s sp', s=n_stk)

        # -> [bs, emb, n_stk]
        sparse_feas_from_dense = dense_fea.max(3)[0]

        # -> [bs, emb, n_stk]
        union_sparse = torch.cat([sparse_fea, sparse_feas_from_dense], dim=1)
        union_sparse = self.encoder(union_sparse)

        return union_sparse


class SDGraphEncoder(nn.Module):
    def __init__(self,
                 sparse_in, sparse_out, dense_in, dense_out,  # 输入输出维度
                 sp_coor_channel,
                 dn_coor_channel,
                 sp_near=2, dn_near=10,  # 更新sdgraph的两个GCN中邻近点数目
                 sample_type='down_sample',  # 采样类型
                 with_time=False, time_emb_dim=0,  # 是否附加时间步
                 dropout=0.4
                 ):
        """
        :param sample_type: [down_sample, up_sample, none]
        """
        super().__init__()
        self.with_time = with_time

        self.dense_to_sparse = DenseToSparse(sparse_in, dense_in, dropout)  # 这个不能设为零
        self.sparse_to_dense = SparseToDense(sparse_in, dense_in, dropout)

        self.sparse_update = SparseUpdate(sp_coor_channel, sparse_in + dense_in, sparse_out, sp_near)
        self.dense_update = DenseUpdate(dn_coor_channel, dense_in + sparse_in, dense_out, dn_near)

        self.sample_type = sample_type
        if self.sample_type == 'down_sample':
            self.sample = DownSample(dense_out, dense_out, dropout)  # 这里dropout不能为零
        elif self.sample_type == 'up_sample':
            self.sample = UpSample(dense_out, dense_out, dropout)  # 这里dropout不能为零
        elif self.sample_type == 'none':
            self.sample = nn.Identity()
        else:
            raise ValueError('invalid sample type')

        if self.with_time:
            self.time_mlp_sp = TimeMerge(sparse_out, sparse_out, time_emb_dim, dropout)  # 这里dropout不能为零
            self.time_mlp_dn = TimeMerge(dense_out, dense_out, time_emb_dim, dropout)  # 这里dropout不能为零

    def forward(self, sparse_coor, dense_coor, sparse_fea, dense_fea, time_emb=None):
        """
        :param sparse_coor: [bs, emb, n_stk]
        :param dense_coor: [bs, emb, n_stk, n_stk_pnt]
        :param sparse_fea: [bs, emb, n_stk]
        :param dense_fea: [bs, emb, n_stk, n_stk_pnt]
        :param time_emb: [bs, emb]
        :return: [bs, emb, n_stk], [bs, emb, n_stk, n_stk_pnt]
        """

        # 信息交换
        union_sparse = self.dense_to_sparse(sparse_fea, dense_fea)
        union_dense = self.sparse_to_dense(sparse_fea, dense_fea)

        # 信息更新
        union_sparse = self.sparse_update(sparse_coor, union_sparse)
        union_dense = self.dense_update(dense_coor, union_dense)

        # 采样
        union_dense, dense_coor = self.sample(union_dense, dense_coor)

        # 融合时间步特征
        assert self.with_time ^ (time_emb is None)
        if self.with_time:
            union_sparse = self.time_mlp_sp(union_sparse, time_emb)
            union_dense = self.time_mlp_dn(union_dense, time_emb)

        return union_sparse, union_dense, dense_coor


class SDGraphCls(nn.Module):
    """
    对于每个子模块的输入和输出
    sparse graph 统一使用 [bs, channel, n_stk]
    dense graph 统一使用 [bs, channel, n_stk, n_stk_pnt]
    """
    def __init__(self, n_class: int, channel_in=2, dropout: float = 0.4):
        """
        :param n_class: 总类别数
        """
        super().__init__()
        print('cls valid')

        self.n_stk = global_defs.n_stk
        self.n_stk_pnt = global_defs.n_stk_pnt

        # 各层特征维度
        sparse_l0 = 32 + 16
        sparse_l1 = 128 + 64
        sparse_l2 = 512 + 256

        dense_l0 = 32
        dense_l1 = 128
        dense_l2 = 512

        # sparse_l0 = 16 + 8
        # sparse_l1 = 64 + 32
        # sparse_l2 = 256 + 128
        #
        # dense_l0 = 16
        # dense_l1 = 64
        # dense_l2 = 256

        # 生成初始 sdgraph
        self.point_to_sparse = PointToSparse(channel_in, sparse_l0)
        self.point_to_dense = PointToDense(channel_in, dense_l0)

        # 利用 sdgraph 更新特征
        self.sd1 = SDGraphEncoder(sparse_l0, sparse_l1, dense_l0, dense_l1,
                                  # self.n_stk, self.n_stk_pnt,
                                  dropout=dropout
                                  )

        self.sd2 = SDGraphEncoder(sparse_l1, sparse_l2, dense_l1, dense_l2,
                                  # self.n_stk, self.n_stk_pnt // 2,
                                  dropout=dropout
                                  )

        # 利用输出特征进行分类
        sparse_glo = sparse_l0 + sparse_l1 + sparse_l2
        dense_glo = dense_l0 + dense_l1 + dense_l2
        out_inc = (n_class / (sparse_glo + dense_glo)) ** (1 / 3)

        out_l0 = sparse_glo + dense_glo
        out_l1 = int(out_l0 * out_inc)
        out_l2 = int(out_l1 * out_inc)
        out_l3 = n_class

        self.linear = eu.MLP(0, (out_l0, out_l1, out_l2, out_l3), True, dropout, False)

    def forward(self, xy, mask=None):
        """
        :param xy: [bs, n_stk, n_stk_pnt, 2]
        :param mask: 占位用
        :return: [bs, n_classes]
        """
        bs, n_stk, n_stk_pnt, channel = xy.size()
        assert n_stk == self.n_stk and n_stk_pnt == self.n_stk_pnt and channel == 2

        # 生成初始 sparse graph
        sparse_graph0 = self.point_to_sparse(xy)
        assert sparse_graph0.size()[2] == self.n_stk

        # 生成初始 dense graph
        dense_graph0 = self.point_to_dense(xy)
        assert dense_graph0.size()[2] == n_stk and dense_graph0.size()[3] == n_stk_pnt

        # 交叉更新数据
        sparse_graph1, dense_graph1 = self.sd1(sparse_graph0, dense_graph0)
        sparse_graph2, dense_graph2 = self.sd2(sparse_graph1, dense_graph1)

        # 提取全局特征
        sparse_glo0 = sparse_graph0.max(2)[0]
        sparse_glo1 = sparse_graph1.max(2)[0]
        sparse_glo2 = sparse_graph2.max(2)[0]

        dense_glo0 = dense_graph0.amax((2, 3))
        dense_glo1 = dense_graph1.amax((2, 3))
        dense_glo2 = dense_graph2.amax((2, 3))

        all_fea = torch.cat([sparse_glo0, sparse_glo1, sparse_glo2, dense_glo0, dense_glo1, dense_glo2], dim=1)

        # 利用全局特征分类
        cls = self.linear(all_fea)
        cls = F.log_softmax(cls, dim=1)

        return cls


class SDGraphUNet(nn.Module):
    def __init__(self, channel_in, channel_out, n_stk, n_stk_pnt, dropout=0.0):
        super().__init__()
        print('diff sdgraph valid end snap with attn 3dgcn')

        '''草图参数'''
        self.channel_in = channel_in
        self.channel_out = channel_out
        self.n_stk = n_stk
        self.n_stk_pnt = n_stk_pnt

        '''各层通道数'''
        sparse_l0 = 32
        sparse_l1 = 128
        sparse_l2 = 512

        dense_l0 = 16
        dense_l1 = 64
        dense_l2 = 256

        global_dim = 1024
        time_emb_dim = 256

        '''时间步特征生成层'''
        self.time_encode = TimeEncode(time_emb_dim)

        '''点坐标 -> 初始 sdgraph 生成层'''
        self.point_to_sparse = PointToSparse(channel_in, sparse_l0, with_time=True, time_emb_dim=time_emb_dim)
        self.point_to_dense = PointToDense(channel_in, dense_l0, with_time=True, time_emb_dim=time_emb_dim)

        '''下采样层 × 2'''
        self.sd_down1 = SDGraphEncoder(sparse_l0, sparse_l1, dense_l0, dense_l1,
                                       sparse_l0, channel_in,
                                       sp_near=5, dn_near=60,
                                       sample_type='down_sample',
                                       with_time=True, time_emb_dim=time_emb_dim,
                                       dropout=dropout)

        self.sd_down2 = SDGraphEncoder(sparse_l1, sparse_l2, dense_l1, dense_l2,
                                       sparse_l0, channel_in,
                                       sp_near=5, dn_near=60,
                                       sample_type='down_sample',
                                       with_time=True, time_emb_dim=time_emb_dim,
                                       dropout=dropout)

        '''全局特征生成层'''
        global_in = sparse_l2 + dense_l2
        self.global_linear = eu.MLP(dimension=0,
                                    channels=(global_in, int((global_in * global_dim) ** 0.5), global_dim),
                                    final_proc=True,
                                    dropout=dropout
                                    )

        '''上采样层 × 2'''
        self.sd_up2 = SDGraphEncoder(global_dim + sparse_l2, sparse_l2, global_dim + dense_l2, dense_l2,
                                     sparse_l0, channel_in,
                                     sp_near=5, dn_near=60,
                                     sample_type='up_sample',
                                     with_time=True, time_emb_dim=time_emb_dim,
                                     dropout=dropout)

        self.sd_up1 = SDGraphEncoder(sparse_l1 + sparse_l2, sparse_l1, dense_l1 + dense_l2, dense_l1,
                                     sparse_l0, channel_in,
                                     sp_near=5, dn_near=60,
                                     sample_type='up_sample',
                                     with_time=True, time_emb_dim=time_emb_dim,
                                     dropout=dropout)

        '''最终输出层'''
        final_in = dense_l0 + sparse_l0 + dense_l1 + sparse_l1 + channel_in
        self.final_linear = eu.MLP(dimension=1,
                                   channels=(final_in, int((channel_out * final_in) ** 0.5), channel_out),
                                   final_proc=False,
                                   dropout=dropout)

    def img_size(self):
        return self.n_stk, self.n_stk_pnt, self.channel_out

    def forward(self, xy, time):
        """
        :param xy: [bs, n_stk, n_stk_pnt, channel_in]
        :param time: [bs, ]
        :return: [bs, n_stk, n_stk_pnt, channel_out]
        """
        '''生成时间步特征'''
        time_emb = self.time_encode(time)

        bs, n_stk, n_stk_pnt, channel_in = xy.size()
        assert n_stk == self.n_stk and n_stk_pnt == self.n_stk_pnt and channel_in == self.channel_in

        '''生成初始 sdgraph'''
        sparse_graph_up0 = self.point_to_sparse(xy, time_emb)  # -> [bs, emb, n_stk]
        dense_graph_up0 = self.point_to_dense(xy, time_emb)  # -> [bs, emb, n_point]
        assert sparse_graph_up0.size()[2] == self.n_stk and dense_graph_up0.size()[2] == n_stk and dense_graph_up0.size()[3] == n_stk_pnt

        '''下采样'''
        sp_coor = sparse_graph_up0
        xy = einops.rearrange(xy, 'b s sp c -> b c (s sp)')

        sparse_graph_up1, dense_graph_up1, dn_coor_l1 = self.sd_down1(sp_coor, xy, sparse_graph_up0, dense_graph_up0, time_emb)
        sparse_graph_up2, dense_graph_up2, dn_coor_l2 = self.sd_down2(sp_coor, dn_coor_l1, sparse_graph_up1, dense_graph_up1, time_emb)

        '''获取全局特征'''
        sp_up2_glo = sparse_graph_up2.max(2)[0]
        dn_up2_glo = dense_graph_up2.amax((2, 3))

        fea_global = torch.cat([sp_up2_glo, dn_up2_glo], dim=1)
        fea_global = self.global_linear(fea_global)  # -> [bs, emb]

        '''将 sd_graph 融合全局特征 (直接拼接在后面)'''
        sparse_fit = einops.repeat(fea_global, 'b c -> b c s', s=self.n_stk)
        sparse_graph_down2 = torch.cat([sparse_graph_up2, sparse_fit], dim=1)

        dense_fit = einops.repeat(fea_global, 'b c -> b c s sp', s=dense_graph_up2.size(2), sp=dense_graph_up2.size(3))
        dense_graph_down2 = torch.cat([dense_graph_up2, dense_fit], dim=1)

        '''上采样并融合UNet下采样阶段对应特征'''
        sparse_graph_down1, dense_graph_down1, _ = self.sd_up2(sp_coor, dn_coor_l2, sparse_graph_down2, dense_graph_down2, time_emb)  # -> [bs, sp_l2, n_stk], [bs, dn_l2, n_pnt]

        sparse_graph_down1 = torch.cat([sparse_graph_down1, sparse_graph_up1], dim=1)
        dense_graph_down1 = torch.cat([dense_graph_down1, dense_graph_up1], dim=1)

        sparse_graph_down0, dense_graph_down0, _ = self.sd_up1(sp_coor, dn_coor_l1, sparse_graph_down1, dense_graph_down1, time_emb)

        sparse_graph = torch.cat([sparse_graph_down0, sparse_graph_up0], dim=1)
        dense_graph = torch.cat([dense_graph_down0, dense_graph_up0], dim=1)

        '''将sparse graph及xy转移到dense graph并输出'''
        sparse_graph = einops.repeat(sparse_graph, 'b c s -> b c s sp', sp=self.n_stk_pnt)
        xy = xy.view(bs, channel_in, n_stk, n_stk_pnt)  # -> [bs, channel, n_stk, n_stk_pnt]

        dense_graph = torch.cat([dense_graph, sparse_graph, xy], dim=1)
        dense_graph = dense_graph.view(bs, dense_graph.size(1), self.n_stk * self.n_stk_pnt)

        noise = self.final_linear(dense_graph)  # -> [bs, channel_out, n_stk * n_stk_pnt]
        noise = einops.rearrange(noise, 'b c (s sp) -> b s sp c', s=self.n_stk, sp=self.n_stk_pnt)

        return noise


class SDGraphEmbedding(nn.Module):
    """
    对于每个子模块的输入和输出
    sparse graph 统一使用 [bs, channel, n_stk]
    dense graph 统一使用 [bs, channel, n_stk, n_stk_pnt]
    """
    def __init__(self, channel_out, n_stk, n_stk_pnt, channel_in=2, sp_near=5, dn_near=50, dropout: float = 0.4):
        """
        :param n_class: 总类别数
        """
        super().__init__()
        print('create sdgraph with attention encoder')

        self.n_stk = n_stk
        self.n_stk_pnt = n_stk_pnt

        # 各层特征维度
        sparse_l0 = 32 + 16
        sparse_l1 = 128 + 64
        sparse_l2 = 512 + 256

        dense_l0 = 32
        dense_l1 = 128
        dense_l2 = 512

        # sparse_l0 = 16 + 8
        # sparse_l1 = 64 + 32
        # sparse_l2 = 256 + 128
        #
        # dense_l0 = 16
        # dense_l1 = 64
        # dense_l2 = 256

        # 生成初始 sdgraph
        self.point_to_sparse = PointToSparse(channel_in, sparse_l0)
        self.point_to_dense = PointToDense(channel_in, dense_l0)

        # 利用 sdgraph 更新特征
        self.sd1 = SDGraphEncoder(sparse_l0, sparse_l1, dense_l0, dense_l1,
                                  sparse_l0, channel_in,
                                  sp_near, dn_near,
                                  dropout=dropout,
                                  )

        self.sd2 = SDGraphEncoder(sparse_l1, sparse_l2, dense_l1, dense_l2,
                                  sparse_l0, channel_in,
                                  sp_near, dn_near,
                                  dropout=dropout,
                                  )

        # 利用输出特征进行分类
        sparse_glo = sparse_l0 + sparse_l1 + sparse_l2
        dense_glo = dense_l0 + dense_l1 + dense_l2

        out_l0 = sparse_glo + dense_glo
        out_l1 = int((out_l0 * channel_out) ** 0.5)
        out_l2 = channel_out

        self.linear = eu.MLP(0, (out_l0, out_l1, out_l2), True, dropout, False)

    def forward(self, xy, mask=None):
        """
        :param xy: [bs, n_stk, n_stk_pnt, 2]
        :param mask: 占位用
        :return: [bs, n_classes]
        """
        bs, n_stk, n_stk_pnt, channel = xy.size()
        assert n_stk == self.n_stk and n_stk_pnt == self.n_stk_pnt and channel == 2

        # 生成初始 sparse graph
        sparse_graph0 = self.point_to_sparse(xy)
        assert sparse_graph0.size()[2] == self.n_stk

        # 生成初始 dense graph
        dense_graph0 = self.point_to_dense(xy)
        assert dense_graph0.size()[2] == n_stk and dense_graph0.size()[3] == n_stk_pnt

        # 交叉更新数据
        sp_coor = sparse_graph0
        dn_coor_l0 = einops.rearrange(xy, 'b s sp c -> b c (s sp)')

        sparse_graph1, dense_graph1, dn_coor_l1 = self.sd1(sp_coor, dn_coor_l0, sparse_graph0, dense_graph0)
        sparse_graph2, dense_graph2, _ = self.sd2(sp_coor, dn_coor_l1, sparse_graph1, dense_graph1)

        # 提取全局特征
        sparse_glo0 = sparse_graph0.max(2)[0]
        sparse_glo1 = sparse_graph1.max(2)[0]
        sparse_glo2 = sparse_graph2.max(2)[0]

        dense_glo0 = dense_graph0.amax((2, 3))
        dense_glo1 = dense_graph1.amax((2, 3))
        dense_glo2 = dense_graph2.amax((2, 3))

        all_fea = torch.cat([sparse_glo0, sparse_glo1, sparse_glo2, dense_glo0, dense_glo1, dense_glo2], dim=1)

        # 利用全局特征分类
        emb_out = self.linear(all_fea)
        return emb_out


def test():
#     bs = 3
#     atensor = torch.rand([bs, 2, global_defs.n_skh_pnt]).cuda()
#     t1 = torch.randint(0, 1000, (bs,)).long().cuda()
#
#     # classifier = SDGraphSeg2(2, 2).cuda()
#     # cls11 = classifier(atensor, t1)
#
#     classifier = SDGraphCls2(10).cuda()
#     cls11 = classifier(atensor)
#
#     print(cls11.size())
#
#     print('---------------')


    bs = 3
    atensor = torch.rand([bs, 12, 32, 2])
    t1 = torch.randint(0, 1000, (bs,)).long()

    classifier_unet = SDGraphUNet(2, 2)
    cls11 = classifier_unet(atensor, t1)
    print(cls11.size())

    classifier_cls = SDGraphCls(10)
    cls12 = classifier_cls(atensor)
    print(cls12.size())

    print('---------------')








if __name__ == '__main__':
    test()
    print('---------------')






